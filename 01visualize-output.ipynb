{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Путь к файлу логов\n",
    "log_file_path = '/home/ir739wb/ilyarekun/nn_DeiT_25/fbkrep/deit/deit_output.log'\n",
    "\n",
    "# Списки для хранения данных\n",
    "epochs = []\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "# Регулярные выражения для извлечения данных\n",
    "epoch_pattern = r'Epoch: \\[(\\d+)\\]'  # Номер эпохи\n",
    "train_loss_pattern = r'loss: [\\d.]+ \\(([\\d.]+)\\)'  # Средний train loss в скобках\n",
    "val_metrics_pattern = r'\\* Acc@1 ([\\d.]+) Acc@5 [\\d.]+ loss ([\\d.]+)'  # Val accuracy и val loss\n",
    "\n",
    "# Чтение файла логов\n",
    "try:\n",
    "    with open(log_file_path, 'r') as file:\n",
    "        current_epoch = None\n",
    "        for line in file:\n",
    "            # Поиск номера эпохи\n",
    "            epoch_match = re.search(epoch_pattern, line)\n",
    "            if epoch_match:\n",
    "                current_epoch = int(epoch_match.group(1))\n",
    "\n",
    "            # Поиск train loss (берем только последнее значение эпохи)\n",
    "            train_loss_match = re.search(train_loss_pattern, line)\n",
    "            if train_loss_match and current_epoch is not None:\n",
    "                # Проверяем, что это последняя строка батча в эпохе\n",
    "                if '[327/328]' in line:  # Предполагаем, что 328 — общее число батчей\n",
    "                    train_loss = float(train_loss_match.group(1))\n",
    "                    if current_epoch not in epochs:  # Добавляем только один раз на эпоху\n",
    "                        epochs.append(current_epoch)\n",
    "                        train_losses.append(train_loss)\n",
    "\n",
    "            # Поиск val loss и val accuracy\n",
    "            val_metrics_match = re.search(val_metrics_pattern, line)\n",
    "            if val_metrics_match and current_epoch is not None:\n",
    "                val_accuracy = float(val_metrics_match.group(1))  # Acc@1\n",
    "                val_loss = float(val_metrics_match.group(2))      # loss\n",
    "                # Добавляем только если эпоха уже записана в train_losses\n",
    "                if current_epoch in epochs and len(val_losses) < len(epochs):\n",
    "                    val_losses.append(val_loss)\n",
    "                    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    # Проверка, что данные были извлечены\n",
    "    if not epochs:\n",
    "        print(\"Не удалось найти данные в файле логов. Проверьте путь к файлу или его формат.\")\n",
    "    else:\n",
    "        # Построение графиков\n",
    "        plt.figure(figsize=(12, 5))\n",
    "\n",
    "        # График для train и val loss\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(epochs, train_losses, label='Train Loss', color='blue')\n",
    "        plt.plot(epochs, val_losses, label='Val Loss', color='red')\n",
    "        plt.xlabel('Эпоха')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Train и Val Loss')\n",
    "        plt.legend()\n",
    "\n",
    "        # График для val accuracy\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(epochs, val_accuracies, label='Val Accuracy', color='green')\n",
    "        plt.xlabel('Эпоха')\n",
    "        plt.ylabel('Accuracy (%)')\n",
    "        plt.title('Val Accuracy')\n",
    "        plt.legend()\n",
    "\n",
    "        # Отображение графиков\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Файл {log_file_path} не найден. Проверьте путь.\")\n",
    "except Exception as e:\n",
    "    print(f\"Произошла ошибка: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Путь к файлу логов\n",
    "log_file_path = '/home/ir739wb/ilyarekun/nn_DeiT_25/fbkrep/deit/deit_output.log'\n",
    "\n",
    "# Списки для хранения данных\n",
    "epochs = []\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "# Регулярные выражения для извлечения данных\n",
    "epoch_pattern = r'Epoch: \\[(\\d+)\\]'  # Номер эпохи\n",
    "train_loss_pattern = r'Averaged stats: lr: [\\d.e-]+  loss: [\\d.]+ \\(([\\d.]+)\\)'  # Средний train loss\n",
    "val_metrics_pattern = r'\\* Acc@1 ([\\d.]+) Acc@5 [\\d.]+ loss ([\\d.]+)'  # Val accuracy и val loss\n",
    "\n",
    "# Чтение файла логов\n",
    "try:\n",
    "    with open(log_file_path, 'r') as file:\n",
    "        current_epoch = None\n",
    "        for line in file:\n",
    "            # Поиск номера эпохи\n",
    "            epoch_match = re.search(epoch_pattern, line)\n",
    "            if epoch_match:\n",
    "                current_epoch = int(epoch_match.group(1))\n",
    "\n",
    "            # Поиск train loss из Averaged stats\n",
    "            train_loss_match = re.search(train_loss_pattern, line)\n",
    "            if train_loss_match and current_epoch is not None:\n",
    "                train_loss = float(train_loss_match.group(1))\n",
    "                if current_epoch not in epochs:  # Добавляем только один раз на эпоху\n",
    "                    epochs.append(current_epoch)\n",
    "                    train_losses.append(train_loss)\n",
    "\n",
    "            # Поиск val loss и val accuracy\n",
    "            val_metrics_match = re.search(val_metrics_pattern, line)\n",
    "            if val_metrics_match and current_epoch is not None:\n",
    "                val_accuracy = float(val_metrics_match.group(1))  # Acc@1\n",
    "                val_loss = float(val_metrics_match.group(2))      # loss\n",
    "                # Добавляем только если эпоха уже записана в train_losses\n",
    "                if current_epoch in epochs and len(val_losses) < len(epochs):\n",
    "                    val_losses.append(val_loss)\n",
    "                    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    # Проверка, что данные были извлечены\n",
    "    if not epochs:\n",
    "        print(\"Не удалось найти данные в файле логов. Проверьте путь к файлу или его формат.\")\n",
    "    else:\n",
    "        # Построение графиков\n",
    "        plt.figure(figsize=(12, 5))\n",
    "\n",
    "        # График для train и val loss\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(epochs, train_losses, label='Train Loss', color='blue')\n",
    "        plt.plot(epochs, val_losses, label='Val Loss', color='red')\n",
    "        plt.xlabel('Эпоха')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Train и Val Loss')\n",
    "        plt.legend()\n",
    "\n",
    "        # График для val accuracy\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(epochs, val_accuracies, label='Val Accuracy', color='green')\n",
    "        plt.xlabel('Эпоха')\n",
    "        plt.ylabel('Accuracy (%)')\n",
    "        plt.title('Val Accuracy')\n",
    "        plt.legend()\n",
    "\n",
    "        # Отображение графиков\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Файл {log_file_path} не найден. Проверьте путь.\")\n",
    "except Exception as e:\n",
    "    print(f\"Произошла ошибка: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dot -V\n",
    "%pip install graphviz\n",
    "from graphviz import Digraph\n",
    "\n",
    "dot = Digraph(comment='DistilledVisionTransformer', format='png')\n",
    "\n",
    "# Input\n",
    "dot.node('Input', 'Input Image\\n[batch_size, 3, 224, 224]')\n",
    "\n",
    "# Patch Embedding\n",
    "dot.node('PatchEmbed', 'Patch Embedding\\nConv2d(3, 192, k=16, s=16)\\n→ [batch_size, 196, 192]')\n",
    "dot.edge('Input', 'PatchEmbed')\n",
    "\n",
    "# Add Tokens\n",
    "dot.node('Tokens', 'Add [CLS] and [DIST] Tokens\\n→ [batch_size, 198, 192]')\n",
    "dot.edge('PatchEmbed', 'Tokens')\n",
    "\n",
    "# Positional Embeddings\n",
    "dot.node('PosEmbed', 'Add Positional Embeddings')\n",
    "dot.edge('Tokens', 'PosEmbed')\n",
    "\n",
    "# Positional Dropout\n",
    "dot.node('PosDrop', 'Positional Dropout (p=0.0)')\n",
    "dot.edge('PosEmbed', 'PosDrop')\n",
    "\n",
    "# Transformer Blocks\n",
    "dot.node('Transformer', 'Transformer Blocks (x12)\\nEach: LayerNorm → Attention (3 heads) → DropPath → LayerNorm → MLP (192→768→192, GELU)')\n",
    "dot.edge('PosDrop', 'Transformer')\n",
    "\n",
    "# Final Normalization\n",
    "dot.node('FinalNorm', 'Final LayerNorm')\n",
    "dot.edge('Transformer', 'FinalNorm')\n",
    "\n",
    "# Classification Heads\n",
    "dot.node('CLS', 'Extract [CLS] Token\\n(index 0)')\n",
    "dot.node('DIST', 'Extract [DIST] Token\\n(index 1)')\n",
    "dot.node('Head', 'head\\nLinear(192→100)')\n",
    "dot.node('HeadDist', 'head_dist\\nLinear(192→100)')\n",
    "dot.node('Logits', 'Logits\\n[batch_size, 100]')\n",
    "dot.node('DistLogits', 'Distillation Logits\\n[batch_size, 100]')\n",
    "\n",
    "dot.edge('FinalNorm', 'CLS')\n",
    "dot.edge('FinalNorm', 'DIST')\n",
    "dot.edge('CLS', 'Head')\n",
    "dot.edge('DIST', 'HeadDist')\n",
    "dot.edge('Head', 'Logits')\n",
    "dot.edge('HeadDist', 'DistLogits')\n",
    "\n",
    "# Render\n",
    "dot.render('deit_architecture', view=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Установка зависимостей\n",
    "%pip install timm graphviz torch matplotlib numpy\n",
    "\n",
    "# Импорт библиотек с проверкой версий\n",
    "try:\n",
    "    import torch\n",
    "    import timm\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"timm version: {timm.__version__}\")\n",
    "except ModuleNotFoundError as e:\n",
    "    print(f\"Ошибка импорта: {e}\")\n",
    "    print(\"Убедитесь, что все библиотеки установлены:\")\n",
    "    print(\"  - conda install pytorch torchvision -c pytorch\")\n",
    "    print(\"  - pip install timm graphviz matplotlib numpy\")\n",
    "    raise\n",
    "\n",
    "# Загрузка модели\n",
    "try:\n",
    "    model = timm.create_model('deit_tiny_distilled_patch16_224', pretrained=True, num_classes=100)\n",
    "    model.eval()\n",
    "    print(\"Модель успешно загружена\")\n",
    "except Exception as e:\n",
    "    print(f\"Ошибка при загрузке модели: {e}\")\n",
    "    raise\n",
    "\n",
    "# Создание случайного входного изображения\n",
    "input_image = torch.randn(1, 3, 224, 224)\n",
    "print(\"Случайный входной тензор создан:\", input_image.shape)\n",
    "\n",
    "# Пример forward pass (для проверки)\n",
    "with torch.no_grad():\n",
    "    output = model(input_image)\n",
    "    print(\"Выход модели:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get patch embedding output\n",
    "with torch.no_grad():\n",
    "    patch_embed_output = model.patch_embed.proj(input_image)  # [1, 192, 14, 14]\n",
    "\n",
    "# Visualize the first channel\n",
    "feature_map = patch_embed_output[0, 0].cpu().numpy()\n",
    "plt.imshow(feature_map, cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.title('Patch Embedding Feature Map (Channel 0)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify Attention class to store attention weights\n",
    "class CustomAttention(torch.nn.Module):\n",
    "    def __init__(self, attn_module):\n",
    "        super().__init__()\n",
    "        self.attn = attn_module\n",
    "        self.last_attn = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.attn(x)\n",
    "        if hasattr(self.attn, 'last_attn'):\n",
    "            self.last_attn = self.attn.last_attn\n",
    "        return x\n",
    "\n",
    "# Apply to all blocks\n",
    "for block in model.blocks:\n",
    "    block.attn = CustomAttention(block.attn)\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    output = model(input_image)\n",
    "\n",
    "# Visualize attention map from the first block, first head\n",
    "attn_map = model.blocks[0].attn.last_attn[0, 0].cpu().numpy()  # [198, 198]\n",
    "plt.imshow(attn_map, cmap='hot', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.title('Attention Map (Block 0, Head 0)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get weights from the first MLP's fc1 layer\n",
    "weights = model.blocks[0].mlp.fc1.weight.detach().cpu().numpy().flatten()\n",
    "\n",
    "# Plot histogram\n",
    "plt.hist(weights, bins=50)\n",
    "plt.title('Weight Distribution of MLP fc1 (Block 0)')\n",
    "plt.xlabel('Weight Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
